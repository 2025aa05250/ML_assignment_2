{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5da392e6-1ffc-4944-9e08-08adc94ddbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 1. Import Libraries\n",
    "# ================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, precision_score,\n",
    "    recall_score, f1_score, matthews_corrcoef\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# 2. Load & Data\n",
    "# ================================\n",
    "# Replace 'stellar.csv' with your dataset path\n",
    "df = pd.read_csv(\"D:\\Praveer MTech\\Course\\Assignment\\ML\\Assignment 2\\star_classification.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "891c8027-2ef3-4957-ac98-5516108b692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the data\n",
    "# Drop duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Handle missing values (simple strategy: drop rows with NA)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Drop irrelevant identifiers (unique IDs that donâ€™t help classification)\n",
    "id_columns = [\"run_ID\", \"rerun_ID\", \"cam_col\", \"field_ID\",\n",
    "              \"spec_obj_ID\", \"plate\", \"MJD\", \"fiber_ID\"]\n",
    "df.drop(columns=id_columns, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7953c989-d60e-4166-b868-8ba1d69a6dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance ranking:\n",
      "     Feature         Score\n",
      "8  redshift  83429.418967\n",
      "6         i   8282.343545\n",
      "5         r   4584.533364\n",
      "2     delta    217.588357\n",
      "0    obj_ID    122.542455\n",
      "7         z     32.328308\n",
      "3         u     30.445339\n",
      "4         g     25.962524\n",
      "1     alpha     21.948822\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing\n",
    "# Encode target labels\n",
    "label_enc = LabelEncoder()\n",
    "df[\"class\"] = label_enc.fit_transform(df[\"class\"])  # galaxy=0, star=1, quasar=2 (example)\n",
    "\n",
    "X = df.drop(columns=[\"class\"])\n",
    "y = df[\"class\"]\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Feature selection (ANOVA F-test)\n",
    "selector = SelectKBest(score_func=f_classif, k=\"all\")\n",
    "X_selected = selector.fit_transform(X_scaled, y)\n",
    "\n",
    "# Show feature importance scores\n",
    "feature_scores = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Score\": selector.scores_\n",
    "}).sort_values(by=\"Score\", ascending=False)\n",
    "\n",
    "print(\"Feature importance ranking:\\n\", feature_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "566f7753-0745-401a-982a-d7bf4eb29edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train / Test data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03e683be-281b-49df-a307-bda7a04b82e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c64f3e75-126b-4acb-942f-54fc735bd004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:200: UserWarning: [23:53:53] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:782: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "#Train, predict and evaluate\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # For multiclass AUC, use 'ovr'\n",
    "    try:\n",
    "        y_prob = model.predict_proba(X_test)\n",
    "        auc = roc_auc_score(y_test, y_prob, multi_class=\"ovr\")\n",
    "    except:\n",
    "        auc = None\n",
    "    \n",
    "    metrics = {\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"AUC\": auc,\n",
    "        \"Precision\": precision_score(y_test, y_pred, average=\"weighted\"),\n",
    "        \"Recall\": recall_score(y_test, y_pred, average=\"weighted\"),\n",
    "        \"F1\": f1_score(y_test, y_pred, average=\"weighted\"),\n",
    "        \"MCC\": matthews_corrcoef(y_test, y_pred)\n",
    "    }\n",
    "    results.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e142690f-0130-4d9c-904a-0525a13fb6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Comparison:\n",
      "                  Model  Accuracy       AUC  Precision   Recall        F1  \\\n",
      "0  Logistic Regression   0.92840  0.977709   0.929428  0.92840  0.927847   \n",
      "1        Decision Tree   0.96630  0.970101   0.966317  0.96630  0.966308   \n",
      "2                  KNN   0.92690  0.967534   0.927824  0.92690  0.926607   \n",
      "3          Naive Bayes   0.74315  0.938585   0.790269  0.74315  0.690546   \n",
      "4        Random Forest   0.97990  0.995141   0.979807  0.97990  0.979773   \n",
      "5              XGBoost   0.97580  0.995612   0.975695  0.97580  0.975688   \n",
      "\n",
      "        MCC  \n",
      "0  0.872008  \n",
      "1  0.940246  \n",
      "2  0.869237  \n",
      "3  0.537239  \n",
      "4  0.964289  \n",
      "5  0.957009  \n",
      "\n",
      "===== Logistic Regression =====\n",
      "Accuracy : 0.9284\n",
      "AUC      : 0.9777\n",
      "Precision: 0.9294\n",
      "Recall   : 0.9284\n",
      "F1 Score : 0.9278\n",
      "MCC      : 0.8720\n",
      "\n",
      "===== Decision Tree =====\n",
      "Accuracy : 0.9663\n",
      "AUC      : 0.9701\n",
      "Precision: 0.9663\n",
      "Recall   : 0.9663\n",
      "F1 Score : 0.9663\n",
      "MCC      : 0.9402\n",
      "\n",
      "===== KNN =====\n",
      "Accuracy : 0.9269\n",
      "AUC      : 0.9675\n",
      "Precision: 0.9278\n",
      "Recall   : 0.9269\n",
      "F1 Score : 0.9266\n",
      "MCC      : 0.8692\n",
      "\n",
      "===== Naive Bayes =====\n",
      "Accuracy : 0.7431\n",
      "AUC      : 0.9386\n",
      "Precision: 0.7903\n",
      "Recall   : 0.7431\n",
      "F1 Score : 0.6905\n",
      "MCC      : 0.5372\n",
      "\n",
      "===== Random Forest =====\n",
      "Accuracy : 0.9799\n",
      "AUC      : 0.9951\n",
      "Precision: 0.9798\n",
      "Recall   : 0.9799\n",
      "F1 Score : 0.9798\n",
      "MCC      : 0.9643\n",
      "\n",
      "===== XGBoost =====\n",
      "Accuracy : 0.9758\n",
      "AUC      : 0.9956\n",
      "Precision: 0.9757\n",
      "Recall   : 0.9758\n",
      "F1 Score : 0.9757\n",
      "MCC      : 0.9570\n"
     ]
    }
   ],
   "source": [
    "# After training and collecting results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Option 1: Print the whole table (already in your code)\n",
    "print(\"\\nModel Performance Comparison:\\n\", results_df)\n",
    "\n",
    "# Option 2: Print metrics for each model one by one\n",
    "for idx, row in results_df.iterrows():\n",
    "    print(f\"\\n===== {row['Model']} =====\")\n",
    "    print(f\"Accuracy : {row['Accuracy']:.4f}\")\n",
    "    if row['AUC'] is not None:\n",
    "        print(f\"AUC      : {row['AUC']:.4f}\")\n",
    "    else:\n",
    "        print(\"AUC      : Not available\")\n",
    "    print(f\"Precision: {row['Precision']:.4f}\")\n",
    "    print(f\"Recall   : {row['Recall']:.4f}\")\n",
    "    print(f\"F1 Score : {row['F1']:.4f}\")\n",
    "    print(f\"MCC      : {row['MCC']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81a8af2-6ede-4c8f-8086-962ecf5e5dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
